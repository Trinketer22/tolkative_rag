{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cc49cb-f78d-4665-b90b-759e5d900f5a",
   "metadata": {},
   "source": [
    "# RagServer\n",
    "\n",
    "This notebook is how the project began.\n",
    "Initial version was built and tested (barely) right in the notebook.\n",
    "\n",
    "## Current state\n",
    "\n",
    "Current actual project code evolved considerabley, compared to the *All in one cell* version.\n",
    "*All in one* is depricated, buggy, and shouldn't be used, but it may be useful to observe\n",
    "train of thought that brought it to life.\n",
    "\n",
    "However, this notebook can be used as a playground to interact with the individual server components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134616f-3638-48c2-bf76-2212dd7a700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastapi langchain-community langchain-huggingface uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b046709-aa3e-4126-a5d5-3a314029d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from pathlib import Path\n",
    "import sys\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040013e-5114-478f-be70-03706ae4a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.embedding import embedding_service\n",
    "from services.llm import llm_service\n",
    "from services.vector_store import vector_store\n",
    "from services.snippet_cache import snippet_cache\n",
    "from services.reranking import reranker\n",
    "from core.retrieval import retrieve_documents, retrieve_documents_by_headers\n",
    "embedding_service.initialize()\n",
    "vector_store.initialize()\n",
    "snippet_cache.initialize()\n",
    "reranker.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d26b0-c04a-4865-a1ba-061578008c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query.lang_detection import extract_code_from_query\n",
    "explain_query = \"\"\"\n",
    "What the following code does?: ```\n",
    " var count = 0;\n",
    "\n",
    "    var cs = self.beginParseAllowExotic();\n",
    "    do {\n",
    "        // these checks could be cheaper in terms of gas if we just try to parse cs;\n",
    "        // but we want to throw exactly `ERROR_INVALID_C5` (not `9` \"cell underflow\")\n",
    "        var (nBits, nRefs) = cs.remainingBitsAndRefsCount();\n",
    "        assert (nRefs == 2)      throw ERROR_INVALID_C5;\n",
    "        assert (nBits == 32 + 8) throw ERROR_INVALID_C5;\n",
    "\n",
    "        val outAction = lazy OutActionWithSendMessageOnly.fromSlice(cs, {\n",
    "            throwIfOpcodeDoesNotMatch: ERROR_INVALID_C5\n",
    "        });\n",
    "\n",
    "        if (isExternal) {\n",
    "            assert (outAction.sendMode & SEND_MODE_IGNORE_ERRORS) throw ERROR_EXTERNAL_SEND_MESSAGE_MUST_HAVE_IGNORE_ERRORS_SEND_MODE;\n",
    "        }\n",
    "\n",
    "        cs = outAction.prev.beginParseAllowExotic();\n",
    "        count += 1;\n",
    "    } while (!cs.isEmpty());\n",
    "\n",
    "    assert (count <= 255) throw ERROR_INVALID_C5;\n",
    "    return self;```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbc29a-6944-41f9-9645-f2b6561c2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_code_from_query(explain_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1afe17-3027-4cf9-a555-ecea7c7c8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_code_from_query(\"Let's deBuG something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98c94a-e01d-40f1-b8b3-0d3e70be2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_code_from_query(\" \".join([\"many\"] * 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b2a65-a868-460d-b644-d760bca977f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_code_from_query(\"tolk current time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7285f-bc92-4367-ba70-c36e6ab9f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extract_code_from_query(\"What implementations of jetton exist\")\n",
    "#extract_code_from_query(\"How to do something\", settings.COMPLEX_QUERY_INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b891611-423b-46d0-9875-4202be969566",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_loop = \"\"\"\n",
    "do {\n",
    "        var (nBits, nRefs) = cs.remainingBitsAndRefsCount();\n",
    "        assert (nRefs == 2)      throw ERROR_INVALID_C5;\n",
    "        assert (nBits == 32 + 8) throw ERROR_INVALID_C5;\n",
    "    } while (!cs.isEmpty());\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e184993-2646-471e-98ad-7c628050a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_only = \"\"\"\n",
    "do {\n",
    "        // these checks could be cheaper in terms of gas if we just try to parse cs;\n",
    "        // but we want to throw exactly `ERROR_INVALID_C5` (not `9` \"cell underflow\")\n",
    "        var (nBits, nRefs) = cs.remainingBitsAndRefsCount();\n",
    "        assert (nRefs == 2)      throw ERROR_INVALID_C5;\n",
    "        assert (nBits == 32 + 8) throw ERROR_INVALID_C5;\n",
    "\n",
    "        val outAction = lazy OutActionWithSendMessageOnly.fromSlice(cs, {\n",
    "            throwIfOpcodeDoesNotMatch: ERROR_INVALID_C5\n",
    "        });\n",
    "\n",
    "        if (isExternal) {\n",
    "            assert (outAction.sendMode & SEND_MODE_IGNORE_ERRORS) throw ERROR_EXTERNAL_SEND_MESSAGE_MUST_HAVE_IGNORE_ERRORS_SEND_MODE;\n",
    "        }\n",
    "\n",
    "        cs = outAction.prev.beginParseAllowExotic();\n",
    "        count += 1;\n",
    "    } while (!cs.isEmpty());\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972e17f-98ce-41c1-9ec0-d947ab6d5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.retrieval import pull_context\n",
    "test_concept = \"do-while loop\"\n",
    "concepts = ['do-while loop',\n",
    "   'cell parsing with beginParseAllowExotic and fromSlice',\n",
    "   'assert-based error handling',\n",
    "   'remainingBitsAndRefsCount usage',\n",
    "   'send mode validation with SEND_MODE_IGNORE_ERRORS',\n",
    "   'iteration counting with limit check']\n",
    "for concept in concepts:\n",
    "    initial_docs = await vector_store.search(concept)\n",
    "    docs_ranked = await reranker.rerank(initial_docs, f\"Explanation of {concept}\")\n",
    "    print(f\"{concept}\\n{docs_ranked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1fd8e-a69a-4c23-8129-29d987cf1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = str(project_root.resolve() / \".models\")\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        cache_folder=models_path\n",
    ")\n",
    "reranker_model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L6-v2\",model_kwargs={\"cache_folder\": models_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeaf39c-b107-4ab3-a2ec-58d87ad3205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from utils.json import load_json_dump\n",
    "import faiss\n",
    "\n",
    "docs_uniq = {}\n",
    "docs = list(map(lambda doc: Document(id=doc[\"id\"], page_content=doc[\"page_content\"], metadata=doc[\"metadata\"]), load_json_dump(str(project_root / \"rag-data/latest_docs.jsonl\"))))\n",
    "for doc in docs:\n",
    "    docs_uniq[doc.id] = doc\n",
    "clean_docs = list(docs_uniq.values())\n",
    "\n",
    "indexes_path = project_root / \"indexes\"\n",
    "if not indexes_path.exists():\n",
    "    indexes_path.mkdir()\n",
    "full_storage = FAISS.from_documents(clean_docs, embedding=embedder)\n",
    "full_storage.save_local(indexes_path / \"full_separate_snip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9d664-95f1-41b3-a2bd-a518a20716c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.reranking import CrossEncoderRerankerWithScores\n",
    "test_reranker = CrossEncoderRerankerWithScores(model=reranker_model, score_threshold=1.0)\n",
    "test_query = \"do-while loop over OutAction cells\"\n",
    "test_set = full_storage.similarity_search_with_relevance_scores(test_query, k=10)\n",
    "ranked_set = test_reranker.compress_documents(list(map(lambda doc: doc[0], test_set)), test_query)\n",
    "#test_set[6][0].id == ranked_set[0].id\n",
    "test_set[6]\n",
    "ranked_set[1]\n",
    "#test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04184b-efc3-4e18-9317-abb9dae8af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_crumbs = map(lambda doc: Document(page_content=doc.metadata[\"crumbs\"], id=doc.id), clean_docs)\n",
    "top_lvl_idx = FAISS.from_documents(list(doc_crumbs), embedding=embedder)\n",
    "top_lvl_idx.save_local(indexes_path / \"top_level_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d4ccb-893f-47a7-94bf-d5c8c7eb10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile libs/reranker.py\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_core.documents import Document\n",
    "from typing import Sequence, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "class CrossEncoderRerankerWithScores(CrossEncoderReranker):\n",
    "    score_threshold: float = 0\n",
    "    executor: Optional[ThreadPoolExecutor] = None\n",
    "    _owns_executor: bool = False\n",
    "    \n",
    "    def __init__(self, *args, max_workers: int = 4, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Dedicated thread pool for CPU-intensive reranking\n",
    "        if self.executor is not None:\n",
    "            self._owns_executor = True\n",
    "        else:\n",
    "            log.debug(\"Creating separate executor for re-ranker\")\n",
    "            self.executor = ThreadPoolExecutor(\n",
    "                max_workers=max_workers,\n",
    "                thread_name_prefix=\"reranker\"\n",
    "            )\n",
    "        \n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "    ) -> Sequence[Document]:\n",
    "        if len(documents) == 0:\n",
    "            return []\n",
    "            \n",
    "        scores = self.model.score(\n",
    "            [(query, doc.page_content) for doc in documents]\n",
    "        )\n",
    "        \n",
    "        docs_with_scores = list(zip(documents, scores))\n",
    "        docs_with_scores.sort(key=lambda doc: doc[1], reverse=True)\n",
    "\n",
    "        result_docs = []\n",
    "        for (doc, score) in docs_with_scores:\n",
    "            # Since array is sorted already, we continue till first miss.\n",
    "            if score < self.score_threshold:\n",
    "                break\n",
    "            result_docs.append(doc)\n",
    "            \n",
    "        log.debug(f\"Reranked {len(documents)} -> {len(result_docs)} documents\")\n",
    "        return result_docs\n",
    "    \n",
    "    async def acompress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "    ) -> Sequence[Document]:\n",
    "        if len(documents) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Run blocking scoring in thread pool\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(\n",
    "            self.executor,\n",
    "            lambda: self.compress_documents(documents, query)\n",
    "        )\n",
    "    def __del__(self):\n",
    "        \"\"\"Only shutdown if we own the executor.\"\"\"\n",
    "        if self._owns_executor and self.executor:\n",
    "            self.executor.shutdown(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef66ae-db08-4b9e-8d0a-9ae70a745057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker, EmbeddingsFilter, DocumentCompressorPipeline\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from reranker import CrossEncoderRerankerWithScores\n",
    "\n",
    "test_query = \"Implement jetton contract using tolk language\"\n",
    "\n",
    "orig_search = full_storage.similarity_search(test_query, k=20)\n",
    "reranker = CrossEncoderRerankerWithScores(model=reranker_model, top_n=5)\n",
    "#print(orig_search[:5])\n",
    "print(f\"From reranker {reranker.compress_documents(documents=orig_search, query=test_query)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5c5ed-e315-4588-bf58-2136bce3116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_query = \"Why is the sky is blue\"\n",
    "test_encodder = CrossEncoderRerankerWithScores(model=reranker_model,\n",
    "                                               top_n=5,\n",
    "                                               score_threshold=1)\n",
    "test_encodder.compress_documents(query=irrelevant_query, documents=full_storage.similarity_search(irrelevant_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1ff86-8aa6-44dc-adf5-3a1828156e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_res = list(map(lambda doc: doc[0], orig_search))\n",
    "test_encodder.compress_documents(documents=orig_res, query=test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3319d14-1876-4480-a590-ed721ce6ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = reranker_model.score(\n",
    "            [(test_query, doc.page_content) for doc in orig_res]\n",
    "        )\n",
    "with_rerank_scores = list(zip(orig_res, scores))\n",
    "with_rerank_scores.sort(key=lambda doc: doc[1], reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a533a-c6a0-4985-ac3c-56e94b79d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor.compress_documents(list(map(lambda doc: doc[0], orig_search)), \"TOLK language syntax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76484599-17fd-498f-bf3e-58b98ad3f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_docs = compression_retriever.invoke(\"TOLK language syntax\")\n",
    "print(len(result_docs))\n",
    "print(result_docs[1])\n",
    "#compressor.compress_documents(list(map(lambda doc: doc[0], orig_search)), \"TOLK language syntax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d02a34-94d9-462e-89e9-83b53f9f89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever.invoke(\"Why is the sky is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25690e9f-5dbf-459e-b1ab-a68fa64d46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_lvl_idx.save_local(\"top_level_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bfbae-816f-4426-b7ce-23a5d4050ad3",
   "metadata": {},
   "source": [
    "# Initial all in one version\n",
    "\n",
    "Initial version was developed right in the notebook\n",
    "``` python\n",
    "\n",
    "%%writefile ../rag_backend.py\n",
    "from fastapi import FastAPI, HTTPException, Header, Request\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Union, Dict, Any, Literal, Iterable, Callable, Optional, Tuple, Sequence\n",
    "import httpx\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import logging\n",
    "import hashlib\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from libs.reranker import CrossEncoderRerankerWithScores\n",
    "from libs.query_api import chat_completion\n",
    "from utils.json import load_json_dump\n",
    "from libs.openai_proxy import Message, ChatCompletionRequest, forward_to_openai, stream_openai_response\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Configuration – adjust if you changed ports/names\n",
    "CHEAP_MODEL =  \"gpt-4o-mini\"#\"GPT-5.1-Codex-Mini\"# \"gpt-4o-mini\"\n",
    "TOP_MODEL = \"claude-sonnet-4.5\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='tolkative-rag.log',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "log = logging.getLogger(\"rag_backend\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "# Scores for the ms-marco-MiniLM-L6-v2 model.\n",
    "# It produces scores from -10 to 10\n",
    "HIGHLY_RELEVANT = 5.0\n",
    "SOMEWHAT_RELEVANT = 1.0\n",
    "HEADERS_THRESHOLD = 0.45\n",
    "# Global thread pool\n",
    "rag_thread_pool: Optional[ThreadPoolExecutor] = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_models_and_index() -> None:\n",
    "    \"\"\"\n",
    "    Load the embedding model and the FAISS index.\n",
    "    This runs only once, when the server boots.\n",
    "    \"\"\"\n",
    "    global embedder, reranker_model, reranker, storage, headers_storage\n",
    "    global tolk_ctx\n",
    "    global snippets_index\n",
    "    global reranker\n",
    "    global lang_index\n",
    "    # global tolk_grammar\n",
    "    # with open(\"docs-data/languages/tolk/grammar/grammar.js\", encoding=\"utf8\") as grammar_file:\n",
    "    #    tolk_grammar = grammar_file.read()\n",
    "\n",
    "    #tolk_ctx = [f\"Here is the TOLK language grammar {tolk_grammar}\"]\n",
    "    log.info(\"Loading HuggingFace embedding model …\")\n",
    "    embedder = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        cache_folder=\".models\"\n",
    "    )\n",
    "    log.info(\"Loading HuggingFace re-ranker model …\")\n",
    "    reranker_model = HuggingFaceCrossEncoder(\n",
    "        model_name=\"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "        model_kwargs={\"cache_folder\": \".models\"}\n",
    "    )\n",
    "    log.info(\"Loading FAISS index …\")\n",
    "    # The index directory must exist; raise a clear error otherwise.\n",
    "    workdir = os.path.realpath(os.path.dirname(__file__))\n",
    "    index_path = os.path.join(workdir, \"indexes\")\n",
    "    doc_idx_path = os.path.join(index_path, \"full_separate_snip\")\n",
    "    if not os.path.isdir(doc_idx_path):\n",
    "        raise RuntimeError(f\"FAISS index folder not found: {doc_idx_path}\")\n",
    "    # Documents text index\n",
    "    storage = FAISS.load_local(\n",
    "        doc_idx_path,\n",
    "        embedder,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "    # Crumbs index\n",
    "    headers_storage = FAISS.load_local(\n",
    "        os.path.join(index_path, \"top_level_index\"),\n",
    "        embedder,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    \n",
    "    # Snippets hash table for demo purposes\n",
    "    snippets = load_json_dump(os.path.join(workdir,\"rag-data\", \"latest_snippets.jsonl\"))\n",
    "    snippets_index = {}\n",
    "    for snip in snippets:\n",
    "        #if \"id\" in snip:\n",
    "        snippets_index[snip[\"id\"]] = snip\n",
    "    log.debug(\"Instantiating reranker\")\n",
    "    reranker = CrossEncoderRerankerWithScores(model=reranker_model, score_threshold=SOMEWHAT_RELEVANT, executor=get_thread_pool())\n",
    "    log.info(\"Startup finished – model & index ready.\")\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "def shutdown_rag():\n",
    "    _shutdown_thread_pool()\n",
    "\n",
    "def get_thread_pool():\n",
    "    global rag_thread_pool\n",
    "    if rag_thread_pool is None:\n",
    "        log.info(\"Starting thread pool\")\n",
    "        rag_thread_pool = ThreadPoolExecutor(\n",
    "            max_workers= max(1, len(os.sched_getaffinity(0)) - 2),\n",
    "            thread_name_prefix=\"rag_worker\"\n",
    "        )\n",
    "    return rag_thread_pool\n",
    "    \n",
    "def _shutdown_thread_pool():\n",
    "    if rag_thread_pool:\n",
    "        log.info(\"Shutting down thread pool\")\n",
    "        rag_thread_pool.shutdown(wait=True)\n",
    "        rag_thread_pool = None\n",
    "        \n",
    "async def execute_async(operation: Callable):\n",
    "    thread_pool = get_thread_pool()\n",
    "    event_loop = asyncio.get_event_loop()\n",
    "    return await event_loop.run_in_executor(\n",
    "        thread_pool,\n",
    "        operation\n",
    "    )\n",
    "        \n",
    "def get_system_context(user_id: Optional[str] = None) -> str:\n",
    "    context = \"\"\"\n",
    "You are an experienced TON developer.\n",
    "Context is wrapped in <context></context> tags.\n",
    "First cite the used context \"id\",\"doc-url\",\"concept\" for every context provided.\n",
    "Keep close attention to the context provided and don't mix the context from different programming\n",
    "languages when generating code.\n",
    "\n",
    "Provide comprehensive answer to the user request using the context supplied.\n",
    "If no context provided, explicitly indicate that fact and do not respond anything else.\n",
    "\n",
    "When code examples provided in the context satisfy user request:\n",
    "- Return code snippets EXACTLY as they appear in the context\n",
    "- Don't merge snippets comming from different files into a single one, unless explicitly told so.\n",
    "- Do NOT modify, improve, or fix the code without explicit request.\n",
    "- If you must reference code, use EXACT copy-paste\n",
    "    \"\"\"\n",
    "    return Message(role=\"system\", content=context)\n",
    "\n",
    "def embed_query(query: str):\n",
    "    return embedder.embed_query(query)\n",
    "    \n",
    "def all_of(predicates: Iterable[Callable]) -> Callable:\n",
    "    \"\"\"Return a predicate that returns True only if *all* sub‑predicates are True.\"\"\"\n",
    "    return lambda x: all(p(x) for p in predicates)\n",
    "\n",
    "async def retrieve_documents_by_headers(query: str, query_vector: Sequence[float], threshold: float, filter_lambda = None, top_k=5) -> List[Document]:\n",
    "    log.debug(f\"Headers query: {query}\")\n",
    "    doc_batch = await execute_async(\n",
    "        lambda: headers_storage.similarity_search_with_score_by_vector(query_vector, top_k=top_k)\n",
    "    )\n",
    "    header_docs = [\n",
    "        doc[0] for doc in doc_batch\n",
    "        #No re-ranking with headers, since\n",
    "        #retrieve_documents(headers_storage, query, threshold, None, top_k)\n",
    "        if doc[1] >= threshold\n",
    "    ]\n",
    "    if len(header_docs) > 0:\n",
    "        result = storage.get_by_ids(list(map(lambda doc: doc.id, header_docs)))\n",
    "        log.debug(f\"Headers result {result}\")\n",
    "        if filter_lambda is not None:\n",
    "            return list(filter(filter_lambda, result))\n",
    "    return list()\n",
    "\n",
    "async def retrieve_documents(cur_index: FAISS, query: str, query_vector: Sequence[float], threshold: float, filter_lambda = None, top_k=10):\n",
    "    # We retrieve more to re-rank later\n",
    "    initial_docs = await execute_async(\n",
    "        lambda: cur_index.similarity_search_by_vector(query_vector, top_k=top_k * 4)\n",
    "    )\n",
    "    # Filter out by tags prior to re-rank\n",
    "    filtered_docs = initial_docs\n",
    "    if filter_lambda is not None:\n",
    "        filtered_docs = list(filter(filter_lambda, filtered_docs))\n",
    "\n",
    "    if len(filtered_docs) == 0:\n",
    "        return filtered_docs\n",
    "\n",
    "    # We migh want to re-rank the result based on itent, instead of a full query\n",
    "    #rank_query = query if intent is None else intent\n",
    "    log.debug(f\"Ranking query {query}\")\n",
    "    docs_ranked = await reranker.acompress_documents(query=query, documents=filtered_docs)\n",
    "    log.debug(f\"Ranked docs: {docs_ranked}\")\n",
    "    return docs_ranked\n",
    "\n",
    "def pack_assintent_context(content: str):\n",
    "    return {\"role\": \"assistant\", \"content\": content}\n",
    "class IntentInfo(BaseModel):\n",
    "    intent: str\n",
    "    concepts: List[str]\n",
    "async def extract_intent(prompt: str) -> IntentInfo:\n",
    "    intent_prompt = \"\"\"\n",
    "Separate the request intent from the user supplied code.\n",
    "Populate the list of explicitly named programming, execution environment concepts or operations necessary for the engineer to know to satisfy the following request.\n",
    "Respond in a following structure {\"intent\": \"<user_intent>\", \"concepts\": [<json list of concepts>]}\n",
    "\n",
    "DONT include into the concepts:\n",
    "- Generic terms like algorithms, blockchain, TON blockchain, etc\n",
    "- Anything related to blockchains other than TON like Ethereum, Solana, Solidity, etc\n",
    "\n",
    "If user is requesting to implement certain functionality, add example request to the concepts. (Jetton contract example, Wrappers example, Unit tests example, etc)\n",
    "\n",
    "Provide response in the raw json format without any markdown.\n",
    "\"\"\" + f\"Request: {prompt}\"\n",
    "    try:\n",
    "        completion_res = await execute_async(lambda: chat_completion([{\"role\":\"user\", \"content\":intent_prompt}], CHEAP_MODEL))\n",
    "        log.debug(f\"Completion result:{completion_res}\")\n",
    "        return IntentInfo(**json.loads(completion_res))\n",
    "    except Exception as e:\n",
    "        log.debug(e)\n",
    "        return IntentInfo(intent=prompt, concepts=[])\n",
    "\n",
    "def fence_code(snip: Document):\n",
    "    lang = \"\"\n",
    "    if \"lang\" in snip[\"metadata\"]:\n",
    "        lang = snip[\"metadata\"][\"lang\"]\n",
    "    return f\"\"\"\n",
    "```{lang}\n",
    "{snip[\"page_content\"]}\n",
    "```\n",
    "    \"\"\"\n",
    "\n",
    "def filter_doc_path(doc, filter_path:str):\n",
    "    doc_content = doc[0] if isinstance(doc, tuple) else doc\n",
    "    return filter_path not in doc_content.metadata[\"from\"]\n",
    "\n",
    "async def render_docs_batch(\n",
    "    docs: List[Document],\n",
    "    skip_languages: Optional[List[str]] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"Async version (though not strictly necessary for dict lookups).\"\"\"\n",
    "    \n",
    "    if not docs:\n",
    "        return []\n",
    "    \n",
    "    doc_snippet_mapping = {}\n",
    "    all_snippet_ids = set()\n",
    "    \n",
    "    for doc in docs:\n",
    "        snippet_refs = doc.metadata.get(\"snippets\", [])\n",
    "        doc_snippet_mapping[doc.id] = snippet_refs\n",
    "        all_snippet_ids.update(snip_ref[\"id\"] for snip_ref in snippet_refs)\n",
    "    \n",
    "    log.debug(f\"Batch rendering {len(docs)} docs with {len(all_snippet_ids)} unique snippets\")\n",
    "    \n",
    "    # Batch fetch (could be async if fetching from database)\n",
    "    def _batch_fetch_snippets():\n",
    "        snippets_cache = {}\n",
    "        missing_snippets = []\n",
    "        for snip_id in all_snippet_ids:\n",
    "            if snip_id in snippets_index:\n",
    "                snippet_obj = snippets_index[snip_id]\n",
    "                if skip_languages is None or snippet_obj[\"metadata\"][\"lang\"] not in skip_languages:\n",
    "                    snippets_cache[snip_id] = snippet_obj\n",
    "            else:\n",
    "                missing_snippets.append(snip_id)\n",
    "        \n",
    "        return snippets_cache, missing_snippets\n",
    "    \n",
    "    # Run in thread pool (useful if snippets_index becomes I/O later)\n",
    "    snippets_cache, missing_snippets = _batch_fetch_snippets() #await execute_async(_batch_fetch_snippets)\n",
    "    \n",
    "    if missing_snippets:\n",
    "        log.warning(f\"Missing {len(missing_snippets)} snippets\")\n",
    "    \n",
    "    # Render synchronously (fast string operations)\n",
    "    rendered_docs = [\n",
    "        _render_single_doc(doc, doc_snippet_mapping[doc.id], snippets_cache)\n",
    "        for doc in docs\n",
    "    ]\n",
    "    \n",
    "    return rendered_docs\n",
    "def _build_doc_url(doc_path: str) -> str:\n",
    "    \"\"\"Extract documentation URL from file path.\"\"\"\n",
    "    if \"docs-data\" not in doc_path:\n",
    "        return \"\"\n",
    "    \n",
    "    parsed_path = doc_path.split('/')[1:]\n",
    "    file_path = parsed_path[-1].split(\".\")[0]\n",
    "    parsed_path[-1] = file_path\n",
    "    \n",
    "    return f'doc-url=\"https://docs.ton.org/{\"/\".join(parsed_path)}\"'\n",
    "def _render_single_doc(\n",
    "    doc: Document,\n",
    "    snippet_refs: List[Dict],\n",
    "    snippets_cache: Dict[str, Any]\n",
    ") -> str:\n",
    "    \"\"\"Render a single document using pre-fetched snippets.\"\"\"\n",
    "    \n",
    "    total_delta = len(doc.metadata.get(\"concept\", \"\")) + 2\n",
    "    doc_content = doc.page_content\n",
    "    \n",
    "    for snip_ref in snippet_refs:\n",
    "        snip_id = snip_ref[\"id\"]\n",
    "        \n",
    "        # No lookup! Just dictionary access to pre-fetched data\n",
    "        snippet_obj = snippets_cache.get(snip_id)\n",
    "        if not snippet_obj:\n",
    "            continue\n",
    "            \n",
    "        fenced_block = fence_code(snippet_obj)\n",
    "        start_idx = total_delta + snip_ref[\"pos\"]\n",
    "        doc_content = doc_content[:start_idx] + fenced_block + doc_content[start_idx:]\n",
    "        total_delta += len(fenced_block)\n",
    "    \n",
    "    # Build context string\n",
    "    doc_url = _build_doc_url(doc.metadata.get(\"from\", \"\"))\n",
    "    # Don't polute context with the empty tags\n",
    "    if len(doc_content.strip()) > 0:\n",
    "        return f'<context id=\"{doc.id}\" orig-doc=\"{doc.metadata[\"from\"]}\" concept=\"{doc.metadata[\"crumbs\"]}\" {doc_url}>{doc_content}</context>'\n",
    "    return \"\"\n",
    "\n",
    "# Going one hop recursive, to guarantee the context order.\n",
    "def add_uniq_context(chunks:List[Document], watch_set: set, cur_depth = 0, max_depth = 1):\n",
    "    uniq_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.id not in watch_set:\n",
    "            watch_set.add(chunk.id)\n",
    "            uniq_chunks.append(chunk)\n",
    "            ref_ids = chunk.metadata[\"child_nodes\"] + chunk.metadata[\"references\"]\n",
    "            if len(ref_ids) > 0 and cur_depth + 1 <= max_depth:\n",
    "                log.debug(f\"Fetching child docs from {chunk}\")\n",
    "                log.debug(f\"Direct children {len(chunk.metadata['child_nodes'])}\")\n",
    "                log.debug(f\"References {len(chunk.metadata['references'])}\")\n",
    "                child_docs = add_uniq_context(storage.get_by_ids(ref_ids), watch_set, cur_depth + 1)\n",
    "                log.debug(f\"Fetched {child_docs}\")\n",
    "                uniq_chunks.extend(child_docs)\n",
    "    return uniq_chunks\n",
    "\n",
    "\n",
    "class ContextResponse(BaseModel):\n",
    "    context: Message\n",
    "    system: Optional[Message] = None,\n",
    "    intent: IntentInfo\n",
    "    \n",
    "'''\n",
    "def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity.\"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "'''\n",
    "\n",
    "def detect_language_from_keywords(intent: str, concepts: List[str]) -> Tuple[Optional[Callable], Optional[Sequence[str]]]:\n",
    "    \"\"\"\n",
    "    Simple language detection from keywords.\n",
    "    \"\"\"\n",
    "    skip_languages: Optional[Sequence] = None\n",
    "    filter_lambda: Optional[Callable] = None\n",
    "    has_tolk = False\n",
    "    has_func = False\n",
    "    text = f\"{intent} {' '.join(concepts)}\".lower().split()\n",
    "\n",
    "    for tok in text:\n",
    "        if tok == \"tolk\":\n",
    "            has_tolk = True\n",
    "        elif tok == \"func\":\n",
    "            has_func = True\n",
    "        if has_tolk and has_func:\n",
    "            break\n",
    "    \n",
    "    if has_tolk and not has_func:\n",
    "        log.debug(\"Excluding FunC context!\")\n",
    "        filter_lambda = lambda doc: filter_doc_path(doc, \"languages/func\")\n",
    "        skip_languages   = [\"func\",\"fift\",\"tact\", \"typescript\", \"ts\"]\n",
    "    elif has_func and not has_tolk:\n",
    "        log.debug(\"Excluding TOLK context!\")\n",
    "        filter_lambda = lambda doc: filter_doc_path(doc, \"languages/tolk\")\n",
    "        skip_languages   = [\"tolk\",\"fift\",\"tact\", \"typescript\", \"ts\"]\n",
    "    return filter_lambda, skip_languages\n",
    "\n",
    "async def pull_context(orig_msgs: List[Message]) -> ContextResponse:\n",
    "    user_msg = orig_msgs[-1].content\n",
    "    context_chunks = []\n",
    "    has_system = any(msg.role == \"system\" for msg in orig_msgs)\n",
    "    system_msg = None\n",
    "    # If there is no system message, put it before the user message\n",
    "    if not has_system:\n",
    "        log.debug(\"Adding system message...\")\n",
    "        system_msg =  get_system_context()\n",
    "\n",
    "    log.debug(f\"Received user message: {user_msg}\")\n",
    "    intent_topics = await extract_intent(user_msg) #IntentInfo(intent=user_msg, concepts=[])\n",
    "    log.debug(f\"Topics from intent: {intent_topics}\")\n",
    "\n",
    "    filter_lambda, skip_languages = detect_language_from_keywords(intent_topics.intent, intent_topics.concepts)\n",
    "\n",
    "    queries_to_embed = [\n",
    "        intent_topics.intent,  # For language detection + header search\n",
    "        user_msg,                 # For text search\n",
    "        *intent_topics.concepts # For concept searches\n",
    "    ]\n",
    "\n",
    "    embeddings = await execute_async(lambda: embedder.embed_documents(queries_to_embed))\n",
    "    \n",
    "    # Unpack embeddings\n",
    "    intent_emb = embeddings[0]\n",
    "    user_msg_emb = embeddings[1]\n",
    "    concept_embs = zip(intent_topics.concepts, embeddings[2:])\n",
    "\n",
    "    # Exclude the others if it belongs to domain with exclusive concepts\n",
    "    # Otherwise LLM will produce the code with mix of languages\n",
    "   \n",
    "    # We can further parellalize all the retrieval, but that's a problem for another day\n",
    "    id_set = set()\n",
    "    context = add_uniq_context(\n",
    "        await retrieve_documents_by_headers(intent_topics.intent, intent_emb, HEADERS_THRESHOLD, filter_lambda),\n",
    "        id_set\n",
    "    )\n",
    "    log.debug(f\"Context from headers: {context}\")\n",
    "    # Less so for full text and\n",
    "    context_from_texts = add_uniq_context(\n",
    "        await retrieve_documents(storage, user_msg, user_msg_emb, SOMEWHAT_RELEVANT, filter_lambda),\n",
    "        id_set\n",
    "    )\n",
    "    log.debug(f\"Context from texts: {context_from_texts}\")\n",
    "    context.extend(context_from_texts)\n",
    "\n",
    "    for topic, topic_emb in concept_embs:\n",
    "        additional_context = await retrieve_documents(storage, topic, topic_emb, SOMEWHAT_RELEVANT, filter_lambda)\n",
    "        log.debug(f\"From topic {topic} added context {additional_context}\")\n",
    "        context.extend(add_uniq_context(additional_context, id_set))\n",
    "\n",
    "    rendered_docs = await render_docs_batch(context, skip_languages)\n",
    "    rendered_ctx = \"\\n\".join(rendered_docs).strip()\n",
    "    #log.debug(context_msgs\n",
    "    '''\n",
    "    rendered_ctx = \"\\n\".join(\n",
    "        await asyncio.gather(\n",
    "            *[render_doc(doc, skip_languages) for doc in context]\n",
    "        )\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    return ContextResponse(\n",
    "        context=Message(\n",
    "        role=\"user\", content=f\"{rendered_ctx}\\n\\n{user_msg}\"\n",
    "        ),\n",
    "        system = system_msg,\n",
    "        intent = intent_topics\n",
    "    )\n",
    "\n",
    "@app.post(\"/context\", response_model=ContextResponse)\n",
    "async def rag_chat(request: Request):\n",
    "    data = await request.json()\n",
    "    #log.debug(\"Initial context:\")\n",
    "    #log.debug(context_msgs)\n",
    "    messages = list(map(lambda msg: Message(**msg), data[\"messages\"]))\n",
    "    res_ctx = await pull_context(messages)\n",
    "    log.debug(res_ctx)\n",
    "    return res_ctx\n",
    "\n",
    "# Models endpoint (as above)\n",
    "class Model(BaseModel):\n",
    "    id: str\n",
    "    object: str = \"model\"\n",
    "    created: int = 0\n",
    "    owned_by: str = \"my-organization\"\n",
    "\n",
    "class ModelsResponse(BaseModel):\n",
    "    object: str = \"list\"\n",
    "    data: List[Model]\n",
    "\n",
    "@app.get(\"/v1/models\")\n",
    "async def list_models():\n",
    "    return ModelsResponse(\n",
    "        object=\"list\",\n",
    "        data=[\n",
    "            Model(id=\"TOLKative model\", owned_by=\"TON Core\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def proxy_chat_completion(\n",
    "    request: ChatCompletionRequest,\n",
    "    authorization: Optional[str] = Header(None),\n",
    "):\n",
    "    \"\"\"\n",
    "    Proxy endpoint that adds context before forwarding to OpenAI\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Inject context into messages\n",
    "        request_ctx = await pull_context(request.messages)\n",
    "        log.debug(request_ctx)\n",
    "        # Prepare the request for OpenAI\n",
    "        openai_request = request.dict(exclude_unset=True)\n",
    "        openai_request[\"messages\"][-1] = request_ctx.context.dict()\n",
    "        openai_request[\"model\"] = TOP_MODEL\n",
    "        if request_ctx.system is not None:\n",
    "            openai_request[\"messages\"].insert(0, request_ctx.system.dict())\n",
    "\n",
    "        # Determine if streaming\n",
    "        if request.stream:\n",
    "            return StreamingResponse(\n",
    "                stream_openai_response(openai_request, authorization),\n",
    "                media_type=\"text/event-stream\"\n",
    "            )\n",
    "        else:\n",
    "            return await forward_to_openai(openai_request)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "#\n",
    "# Health check\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"service\": \"openai-proxy\"}\n",
    "\n",
    "# Middleware for logging\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request: Request, call_next):\n",
    "    log.debug(f\"Request: {request.method} {request.url.path} {await request.json()}\")\n",
    "    response = await call_next(request)\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601efb58-7225-4fa8-a40c-03e515af725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "def _run_uvicorn():\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=4321, log_level=\"debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90c77a-eb27-4876-a818-14671d2cc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "fastapi_thread = threading.Thread(target=_run_uvicorn, name=\"fastapi_thread\", daemon=True)\n",
    "fastapi_thread.start()\n",
    "print(\"Fastapi running...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c155132-0e7a-4f62-a6b5-22562905e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_models():\n",
    "    return requests.get(\n",
    "        \"http://tolkative-notebook:8000/v1/models\"\n",
    "    ).text\n",
    "def get_context(prompt: str, **additional_params):\n",
    "    default_dict: dict = {\"model\": \"whatever\", \"messages\": [{\"role\": \"user\", \"content\": prompt}]}\n",
    "    return requests.post(\n",
    "            \"http://tolkative-notebook:8000/context\",\n",
    "            json={**default_dict, **additional_params},\n",
    "        ).json()\n",
    "def proxy_request(prompt: str, stream: bool = False):\n",
    "    resp = requests.post(\n",
    "        \"http://tolkative-notebook:8000/v1/chat/completions\",\n",
    "        json={\"stream\": stream, \"model\": \"whatever\", \"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "        stream=stream\n",
    "    )\n",
    "    if not stream:\n",
    "        return resp.json()\n",
    "    \n",
    "    # This is stream implementation for testing purposes only.\n",
    "    data_chunks = []\n",
    "    for chunk in resp:\n",
    "        data_chunks.append(chunk)\n",
    "    return json.loads(data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b88b75-c718-41e6-8ec6-143315c1bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a8e93-f7ab-43e7-9df4-47d5a418bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.retrieval import retrieve_documents_by_headers\n",
    "simple_test_query = \"What are the know issues with FunC?\"\n",
    "#vector_res = await vector_store.search(simple_test_query)\n",
    "#await reranker.rerank(vector_res, simple_test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbbcad7-6823-40b2-885b-367d58ba882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(simple_test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb310bb-1adc-4a8e-a1f7-1700b195119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(\"FunC language design issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668be432-af11-4485-b967-e13cc6e9a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f287f-1e9f-403e-bb0d-385f830acc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ctx = get_context(explain_query)\n",
    "res_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2152f-c85a-4a5f-a369-341f8c353cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(\"write a tolk snippet that checks that current time is within range of last 5 minutes of any hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112fd9c3-544c-4cbe-ba03-00cf574d7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(\"current time tolk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f6436-dba3-4aeb-a77b-b55cf9e96ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(\"Provide a basic jetton using tolk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e3b2b-9ea3-40ba-9adc-e0130a640401",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(\"Provie jetton wallet implementation  in FunC with sharding and unit tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0a26c-f5f8-4156-9fc8-09f8ba83e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(\"write a tolk snippet that checks that current time is within range of last 5 minutes of any hour\", **{\"max_tokens\": 300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4c4de-2755-42e5-b58e-c0ae839a509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(\"write a tolk snippet that checks that current time is within range of last 5 minutes of any hour\", **{\"max_tokens\": 5000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568aa83-3884-4d61-9ab7-5b81be555096",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(\"How to implement sharded contract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730d8bf-2b65-4d73-99c6-3f77ca955362",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(\"tolk get current time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457a88e-135f-4802-a6f7-ba2f5871fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_context(explain_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd1319-4343-4db5-8653-60a96bf684d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_request(\"Why is the sky is blue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc4a69-8673-4640-9712-4c5c0447e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_request(\"Implement a jetton contract using tolk. Modify it in such a way that minter admin is able to lock arbitrary wallet transfers. After update wrappers and unit tests for this scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d91704-3e32-4eba-8b98-40d6a0a80ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_request(\"What PFXDICT instruction actually does?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ed91b-3ace-4362-95f2-ee569f43a2f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "def is_port_open(host=\"127.0.0.1\", port=4321):\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.settimeout(0.5)\n",
    "    result = s.connect_ex((host, port))\n",
    "    s.close()\n",
    "    return result == 0\n",
    "\n",
    "print(\"RAG port open?\", is_port_open())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecba621-7f25-4246-9958-48394f214eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
