{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703273fa-507a-4527-951d-a37361fb5892",
   "metadata": {},
   "source": [
    "# Parse snippets\n",
    "This notebook was intended to build knowlage graph over documentation.\n",
    "## Current maps\n",
    "- Raw synonym map over docs\n",
    "- TOLK AST node types to documents that those are explaining\n",
    "- Raw predicates map over statements in documentation\n",
    "- Instructions documentation generated from tvm-specification.json\n",
    "\n",
    "## Postponed\n",
    "Due to knowlage graph being extremely time consuming(manual labor)\n",
    "approach, it is currently postponed in favor of LLM powered intent extraction to speed up the prototyping.\n",
    "But i still beleive this is a valid approach which can bring massive benifit,\n",
    "compared to LLM powered retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718d3a0-2069-4a24-a893-0d9223a16a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall -y tree-sitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7af54d-36e1-4e96-8b20-51650d18110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tree_sitter==0.21.3 rank-bm25 ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac1d91-389c-4a08-8d4a-7f01649e4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "root_path = pathlib.Path.cwd().parent.resolve()\n",
    "sys.path.insert(0, str(root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d875a-b23d-472a-874b-9b90cd219b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Parser, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af4624-293a-4a43-9001-8a18d5c59360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Language.build_library(\"tolk-tree-sitter.so\", [\"../tree-sitter-tolk/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527318d3-ae09-4fa3-97fc-f8fe748fdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's vectorize all the tolk related documents first\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "def create_doc(doc):\n",
    "    return Document(page_content=doc[\"page_content\"], id=doc[\"id\"], metadata=doc[\"metadata\"])\n",
    "    \n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", cache_folder= str(root_path / \".models\"))\n",
    "#code_embedder = HuggingFaceEmbeddings(model_name=\"microsoft/codebert-base\",\n",
    "#                                      encode_kwargs={\"normalize_embeddings\": True},   # L2‑normalize for FAISS\n",
    "#                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e213844-32f6-4230-a35a-72faa62338af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json_dump(path: str):\n",
    "    parsed_data = []\n",
    "    with open(path, encoding=\"utf8\") as data_file:\n",
    "        for line in data_file:\n",
    "            parsed_data.append(json.loads(line))\n",
    "    return parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531e338-d221-4053-a354-b9eb0039d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.json import load_json_dump\n",
    "parsed_snippets = load_json_dump(\"../rag-data/latest_snippets.jsonl\")\n",
    "parsed_docs = load_json_dump(\"../rag-data/latest_docs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c5bf8-d3bb-42f0-8faf-006aee73e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolk_documents = list(\n",
    "    map(\n",
    "        create_doc, filter(lambda x: \"languages/tolk\" in x[\"metadata\"][\"from\"] and \"changelog\" not in x[\"metadata\"][\"from\"], parsed_docs)\n",
    "    )\n",
    ")\n",
    "#(len(parsed_snippets))\n",
    "tolk_snippets = list(map(create_doc, filter(lambda x: x[\"metadata\"][\"lang\"] == \"tolk\" , parsed_snippets)))\n",
    "print(len(tolk_snippets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dba97-002f-4979-9f08-58ef987ee276",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOLK_LANG = Language(\"../tolk-tree-sitter.so\", \"tolk\")\n",
    "tolk_parser = Parser()\n",
    "tolk_parser.set_language(TOLK_LANG)\n",
    "with open(\"../tolk-contracts/contracts_Tolk/03_notcoin/jetton-utils.tolk\", \"r\") as test_file:\n",
    "    ast = tolk_parser.parse(bytes(test_file.read(), \"utf8\"))\n",
    "print(ast.root_node)\n",
    "ast.root_node.sexp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bcd123-7a89-4f72-901b-12d8fa76e158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concept_set = set()\n",
    "for doc in tolk_documents:\n",
    "    if \"tolk\" in doc.metadata[\"from\"]:\n",
    "        concept_set.add(doc.metadata[\"crumbs\"])\n",
    "concept_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbd8cc-f2d7-4ac6-a7e4-e4ba68830c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from binascii import hexlify\n",
    "import unicodedata\n",
    "# This was born in collaborative effort of LLM and good old manual labor.\n",
    "# Idea is that from documents crumbs (headers hierarchy) we create semantic labesl\n",
    "# That will be later used in knowlage graph\n",
    "with open(\"../rag-data/tolk-semantic-labels.json\", \"r\", encoding=\"utf8\") as heading_semantic:\n",
    "    heading = json.loads(unicodedata.normalize('NFKC', heading_semantic.read()))\n",
    "#for heading_key in heading:\n",
    "#    if \"Overall\" in heading_key:\n",
    "#        print(f\"{heading_key} ({hexlify(bytes(heading_key, \"utf8\"))})\")\n",
    "#print(heading[\"Imports and name resolution>All top‐level symbols must have unique names\"])\n",
    "#print(\"\\n\\n\")\n",
    "for doc in tolk_documents:\n",
    "    crumbs = doc.metadata[\"crumbs\"]\n",
    "    if crumbs in heading:\n",
    "        heading_data = heading[crumbs]\n",
    "        doc.metadata[\"label\"] = heading_data[\"summary\"]\n",
    "        doc.metadata[\"short_desc\"] = heading_data[\"reason\"]\n",
    "    else:\n",
    "        print(f\"{crumbs} ({hexlify(bytes(crumbs, \"utf8\"))})not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2a51b-d764-4c89-a99c-5d35c1d4f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe44924-1538-4ab8-bee7-d525ec732cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_headers = []\n",
    "for doc in tolk_documents:\n",
    "    doc_headers.append(Document(page_content=doc.metadata['crumbs'], metadata={'doc_id': doc.id}))\n",
    "headers_index = FAISS.from_documents(doc_headers, embedding=embedder)#BM25Retriever.from_documents(doc_headers, preprocess_func=word_tokenize, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e0422-1e99-4815-997e-08ab85e00686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from nltk.tokenize import word_tokenize\n",
    "docs_vectors = FAISS.from_documents(tolk_documents, embedding=embedder)\n",
    "docs_bm25 = BM25Retriever.from_documents(tolk_documents, preprocess_func=word_tokenize)\n",
    "code_storage = BM25Retriever.from_documents(tolk_snippets)\n",
    "#with open(\"bm25_dump.json\", \"w\", encoding=\"utf8\") as bm25_out:\n",
    "#    bm25_out.write(docs_bm25.model_dump_json())\n",
    "#code_storage = FAISS.from_documents(tolk_snippets, embedding=code_embedder, normalize_L2=False, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcde45-698e-49d1-8b0b-8f28a9ab9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"output specification\"\n",
    "#docs_vectors.save_local('tolk_only')\n",
    "vector_res = docs_vectors.similarity_search_with_relevance_scores(test_query, k=10)\n",
    "bm25_res = docs_bm25.invoke(test_query)\n",
    "print(f\"Vec:{vector_res}\\n\\n\")\n",
    "print(f\"Headers:{headers_index.similarity_search(test_query)}\\n\\n\")\n",
    "print(f\"BM25:{bm25_res}\\n\\n\")\n",
    "print(f\"CodeVec:{code_storage.invoke(test_query)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd3d1a9-63cc-465b-8b0e-7088340fd7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import ipywidgets as w\n",
    "from IPython.display import clear_output\n",
    "import asyncio\n",
    "from IPython.display import display\n",
    "# What we're going to do here is parse all the TOLK contracts files out there and\n",
    "# Label each unique AST node type to the documentation concept\n",
    "\n",
    "tolk_semantic_map: dict = {}\n",
    "def node_type2query(node_type: str):\n",
    "    return node_type.replace('_', ' ') # Replacing node type\n",
    "async def prompt_human(prompt_text: [str], candidates: [str]):\n",
    "    #print(f\"Node type {node_type}\\nRepr:${node_text}\\nSelect label:\")\n",
    "    paragraphs = \"\"\n",
    "    for paragraph in prompt_text:\n",
    "        paragraphs += f\"<p>{paragraph}</p>\"\n",
    "        \n",
    "    prompt = w.HTML(\n",
    "    value=f\"\"\"\n",
    "    {paragraphs}\n",
    "    <i>You may select more than one or 'Skip' the labeling. Use ⌘ (Mac) / Ctrl (Win/Linux) to add to the selection.</i>\n",
    "    \"\"\",\n",
    "    layout=w.Layout(margin='0 0 5px 0')\n",
    ")\n",
    "    multi = w.SelectMultiple(\n",
    "    options=candidates + ['Skip'],\n",
    "    description=\"Select one or more candidate\",\n",
    "    layout=w.Layout(width='800px'),\n",
    "    rows=20\n",
    "    )\n",
    "    submit = w.Button(description=\"Submit label\", button_style='success', disabled=True)\n",
    "    # Multi-select change event\n",
    "    def _on_selection_change(change):\n",
    "        submit.disabled = len(change['new']) == 0\n",
    "    multi.observe(_on_selection_change, names='value')\n",
    "    \n",
    "    out = w.Output()\n",
    "    box = w.VBox([prompt, multi, submit, out])\n",
    "    display(box)\n",
    "    #print(f\"Got here: {box}\")\n",
    "    loop = asyncio.get_event_loop()  # Get the event loop\n",
    "    done_fut = loop.create_future()  # Create future on the correct loop\n",
    "    selected_value = []\n",
    "\n",
    "    def _on_click(_):\n",
    "        if not done_fut.done():\n",
    "            with out:\n",
    "                #print(\"Got into click handler!\")\n",
    "                #print(f\"Before fut: {done_fut}\")\n",
    "                selected_value.extend(list(multi.value))\n",
    "                loop.call_soon_threadsafe(done_fut.set_result, True)      # resolve the Future\n",
    "                #out.clear_output()\n",
    "                #print(f\"Done fut: {done_fut}\")\n",
    "                #print(\"✔️  Submitted – you may continue.\")\n",
    "                \n",
    "    submit.on_click(_on_click)\n",
    "    await done_fut\n",
    "    box.close()\n",
    "    clear_output(wait=False)\n",
    "    print(f\"{selected_value}\")\n",
    "    return selected_value\n",
    "\n",
    "def add_label(node_type: str, label: str, label_map: dict):\n",
    "    if node_type in label_map:\n",
    "        if label not in label_map[node_type]:\n",
    "            label_map[node_type].append(label)\n",
    "    else:\n",
    "        label_map[node_type] = [label]\n",
    "        \n",
    "async def label_node(node, label_map: dict):\n",
    "    node_type = node.type\n",
    "    if node_type not in label_map:\n",
    "        query = node_type2query(node_type)\n",
    "        concept_mapped = False\n",
    "        print(f\"Checking query: {query}\")\n",
    "        candidates = docs_vectors.similarity_search_with_relevance_scores(query, k=10)\n",
    "        auto_candidates = filter(lambda resp: resp[1] >= 0.25, candidates)\n",
    "        candidates_added = 0\n",
    "        for response in auto_candidates:\n",
    "            auto_label = response[0].metadata[\"label\"]\n",
    "            add_label(node_type, auto_label, label_map)\n",
    "            candidates_added = candidates_added + 1\n",
    "            print(f\"Added auto label {node_type}:{auto_label}\")\n",
    "            #print(response\n",
    "            concept_mapped = candidates_added >= 4\n",
    "        if not concept_mapped:\n",
    "            bm25_picks  = list(map(lambda bm_res: \"(bm25)\" + bm_res.metadata['crumbs'], docs_bm25.invoke(query)))\n",
    "            hand_picked = await prompt_human([f\"<b>Node ast type</b>:{node_type}\",f\"<b>Text</b/>:{node.text}\"], list(map(lambda res: f\"(score: {float(res[1]):.2f}){res[0].metadata['crumbs']}\", candidates)) + bm25_picks)\n",
    "            for picked_crumbs in hand_picked:\n",
    "                crumb_label = picked_crumbs\n",
    "                if picked_crumbs != \"Skip\":\n",
    "                    picked_crumbs = picked_crumbs[picked_crumbs.index(\")\") + 1:]\n",
    "                    crumb_label   = heading[picked_crumbs][\"summary\"]\n",
    "                add_label(node_type, crumb_label, label_map)\n",
    "            \n",
    "async def label_tree(node, label_map: dict):\n",
    "    await label_node(node, label_map)\n",
    "    for child_node in node.named_children:\n",
    "        await label_tree(child_node, label_map)\n",
    "async def label_tolk_semantics():\n",
    "    contracts_path = pathlib.Path(\"tolk-contracts/contracts_Tolk/\")\n",
    "    sources = contracts_path.rglob(\"*.tolk\")\n",
    "    for source_path in sources:\n",
    "        with open(source_path, \"r\") as source_file:\n",
    "            print(f\"Processing {source_path}\")\n",
    "            try:\n",
    "                ast = tolk_parser.parse(bytes(source_file.read(), \"utf8\"))\n",
    "                await label_tree(ast.root_node, tolk_semantic_map)\n",
    "            except Exception as err:\n",
    "                print(f\"Failed to parse file ${source_path} {err}\")\n",
    "                raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd73f7-92e9-4607-ad6e-86787c7073df",
   "metadata": {},
   "outputs": [],
   "source": [
    "await label_tolk_semantics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af9f34-38ed-4b99-89cf-100585c2e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolk_semantic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746ecd3-87ce-4b48-add2-be14b0b33067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../rag-data/tolk_syntax_semantic.json\", \"w\", encoding=\"utf8\") as semantic_out:\n",
    "    semantic_out.write(json.dumps(tolk_semantic_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28a911-a090-4081-9ec9-14e2084791e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_obj(path: str):\n",
    "    with open(path,\"r\", encoding=\"utf8\") as json_input:\n",
    "        return json.loads(json_input.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af895761-2e0e-4d0f-a8a9-e3eda0675c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query_api import chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c0c5a-7de6-4405-ad81-0775ff89ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_triplets(text: str):\n",
    "    system_prompt = f\"\"\"\n",
    "You are an expert knowledge‑graph builder.\n",
    "Read the following passage and output every *explicit* factual relation\n",
    "you can find, as a JSON list of objects with the keys:\n",
    "  - \"subj\": the subject noun (Should be directly identifiable no It/They/Them/etc)\n",
    "  - \"pred\": the predicate (single verb in present tense, e.g. \"USES\", \"EXPLAINS\" keep the upper case single word)\n",
    "  - \"obj\": the single noun should be also directly identifiable\n",
    "  \n",
    "Reply with a raw parsable json (don't enclose response into markdown tags)\n",
    "If a sentence contains more than one relation, output them all.\n",
    "If you cannot find a clear relation, output an empty list [].\n",
    "\"\"\"\n",
    "    messages =[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Passage: {text}\"}\n",
    "    ]\n",
    "    return chat_completion(messages, \"gpt-4o-mini\", 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404303d-5db3-45a5-b874-c2fd9d4b5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def query_instruction(instruction):\n",
    "    system_prompt = \"\"\"\n",
    "You are an expert in TVM assembly language.\n",
    "Based only on the input json, provide detailed instruction description.\n",
    "Use markdown headers to separate the object keys.\n",
    "Summarize long and short description together.\n",
    "If json key is empty, don't mention it.\n",
    "    \"\"\"\n",
    "    messages =[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": json.dumps(instruction)}\n",
    "    ]\n",
    "    return chat_completion(messages, \"gpt-4o-mini\", 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8a2f6-6fc9-4690-bfd7-177cecacc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def query_synonyms(text: str):\n",
    "    system_prompt = \"\"\"\n",
    "You are an expert synonym extraction assistant.\n",
    "Read the following passage and output every group of words that\n",
    "refer to the same entity.\n",
    "Return the result as a JSON list, where each entry has:\n",
    "{\n",
    "  \"entity\":  \"<canonical singular name you choose>\",\n",
    "  \"mentions\": [\"<first occurrence>\", \"<second occurrence>\", ...]\n",
    "}\n",
    "\n",
    "**Rules**\n",
    "1️ Use only the exact words that appear in the input, but **convert each word to its singular form** (e.g. “collections” → “collection”, “set methods” → “set method”).  \n",
    "2️ The **entity** should be the **shortest singular mention** in the group (if there is a tie, pick the one that appears first).  \n",
    "3️ Preserve the original spelling/casing *except* for the plural‑to‑singular change.\n",
    "4️ Do **not** invent synonyms that are not present in the text.  \n",
    "5️ Return **pure JSON** – no introductory text, markdown fences, or explanations.  \n",
    "6️ If a term appears more than once in the same form, list it only once inside the “mentions” array.\n",
    "   Provide response in a form of raw json\n",
    "\"\"\"\n",
    "    messages =[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Passage: {text}\"}\n",
    "    ]\n",
    "    return chat_completion(messages, \"gpt-4o-mini\", 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69375841-6459-4286-ae60-e70bd39174e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_synonyms(\"Contract getters (get methods) are used to retrieve the on-chain information off-chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95ca2d-45de-49ce-b6c0-b49520b662da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_predicates(path: str):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf8\") as predicate_map:\n",
    "            return json.loads(predicate_map.read())\n",
    "    except:\n",
    "        print(\"Predicates not found!\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02d94d-2bb7-4ccb-8f8e-b66448d04b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_parsed = load_predicates(\"tvm-spec/tvm-specification.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2694428-4688-48e1-bf29-0105f32ec059",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(instructions_parsed[\"instructions\"])\n",
    "instructions_map = {}\n",
    "for instruction in instructions_parsed[\"instructions\"]:\n",
    "    instruction_cat = instruction[\"category\"] if \"category\" in instruction else \"Uncategorized\"\n",
    "    # Don't want to everload LLM with complexity.\n",
    "    del instruction[\"category\"]\n",
    "    del instruction[\"sub_category\"]\n",
    "    del instruction[\"layout\"]\n",
    "    if instruction_cat in instructions_map:\n",
    "        instructions_map[instruction_cat].append(instruction)\n",
    "    else:\n",
    "        instructions_map[instruction_cat] = [instruction]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349aa4c-3c09-4ffc-a83f-75edc5d986c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_desc_map = {}\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for cat in instructions_map:\n",
    "    for instruction in tqdm(instructions_map[cat]):\n",
    "        if cat not in instruction_desc_map:\n",
    "            instruction_desc_map[cat] = {}\n",
    "        try:\n",
    "            instruction_desc = await query_instruction(instruction)\n",
    "            instruction_desc_map[cat][instruction[\"name\"]] = instruction_desc\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247a175-bb4f-40b1-b95f-100c02202841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.json import save_json_dict\n",
    "save_json_dict(instruction_desc_map, \"../rag-data/instructions_desc.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a034ace-2058-4d40-a526-0b1ed5bb0411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicates = load_predicates(\"../rag-data/raw_predicates.json\")\n",
    "i = 0\n",
    "for doc in tolk_documents:\n",
    "    if doc.id in predicates:\n",
    "        print(f\"Document {doc.id} is cached!\")\n",
    "        continue\n",
    "    if doc.metadata[\"token_count\"] < 1024:\n",
    "        predicate_res = query_triplets(doc.page_content)\n",
    "        #print(predicate_res)\n",
    "        try:\n",
    "            pred_json = json.loads(predicate_res['choices'][-1]['message']['content'])\n",
    "            predicates[doc.id] = pred_json\n",
    "            i = i + 1\n",
    "            print(f\"Processed doc: {doc.id} {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processsing {doc.id} {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5bfcc6-dec1-4809-8862-0b7106466561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synonyms = load_predicates(\"../rag-data/raw_synonims.json\")\n",
    "i = 0\n",
    "for doc in tolk_documents:\n",
    "    if doc.id in synonyms:\n",
    "        print(f\"Document {doc.id} is cached!\")\n",
    "        continue\n",
    "    \n",
    "    llm_res = query_synonyms(doc.page_content)\n",
    "    #print(predicate_res)\n",
    "    try:\n",
    "        synonym_json = json.loads(llm_res)\n",
    "        synonyms[doc.id] = synonym_json\n",
    "        i = i + 1\n",
    "        print(f\"Processed doc: {doc.id} {i}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processsing {doc.id} {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b6abb-3199-43bd-8233-dd939ea08c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67626b54-e54b-4cce-a2e4-8c68880063f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../rag-data/raw_predicates.json\", \"w\", encoding=\"utf8\") as predicates_file:\n",
    "    predicates_file.write(json.dumps(predicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953dd79-4a10-46aa-bc04-20947e73a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../rag-data/raw_synonyms.json\", \"w\", encoding=\"utf8\") as predicates_file:\n",
    "    predicates_file.write(json.dumps(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ec9ea-abda-4b3f-b214-83f8d68bfd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolk_predicates = load_predicates(\"../rag-data/raw_predicates.json\")\n",
    "def map_by_key(pred: dict, key):\n",
    "    new_map = {}\n",
    "    for doc_id in pred:\n",
    "        for triplet in pred[doc_id]:\n",
    "            pred_key = triplet[key]\n",
    "            if pred_key in new_map:\n",
    "                new_map[pred_key].append(triplet)\n",
    "            else:\n",
    "                new_map[pred_key] = [triplet]\n",
    "    return new_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad7dff-1c52-4505-b156-cbddc0ed242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_map = map_by_key(tolk_predicates, \"subj\")\n",
    "pred_map = map_by_key(tolk_predicates, \"pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124cd69-8b4d-41b3-8ca2-531e114eec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06228ae7-9aab-4565-80a4-8a71e49823fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
