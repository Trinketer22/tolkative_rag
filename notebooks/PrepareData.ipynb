{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c5e5cb-a5e3-4a8f-913f-e49be2e59265",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "This notebook prepares raw data to be later used for vector storage.\n",
    "\n",
    "## Sources:\n",
    "- Documentation (`docs-data`)\n",
    "- Text of standards (`TEPs`)\n",
    "- TVM specification docs (Generated in ParseSnippets)\n",
    "- Some contracts examples from (`tolk-contracts`)\n",
    "\n",
    "## Method\n",
    "\n",
    "Documents are parsed into hierarchial structure based on their heading level.  \n",
    "Snippets larger than 5 characters are removed into separate snippets index.  \n",
    "It is done to not polute vector similarity, so vector search alway performed only  \n",
    "against natural language content.\n",
    "\n",
    "### Document structure\n",
    "Document has the following structure:\n",
    "``` python\n",
    "new_doc = create_document(doc_content, path, {\n",
    "    \"concept\": title,\n",
    "    \"word_count\": len(doc_content.split()),\n",
    "    \"token_count\": (len(doc_content) + 3) // 4, # Rough anthropic estimate\n",
    "    \"from\": path,\n",
    "    \"url_from\": url_from,\n",
    "    \"child_nodes\": [],\n",
    "    \"references\": references.copy(),\n",
    "    \"snippets\": ref_snippets.copy(),\n",
    "    \"crumbs\": \">>\".join(crumbs)\n",
    "})\n",
    "```\n",
    "\n",
    "#### Concept attribute\n",
    "Concept name it explains.\n",
    "Usually just it's header.\n",
    "\n",
    "#### Word and token count\n",
    "\n",
    "Theese are self exlainatory.\n",
    "\n",
    "Token count is rough estimate and\n",
    "not really used anywhere yet, but will\n",
    "be helpfull in the future.\n",
    "\n",
    "#### From attribue\n",
    "Source path relative to the project root\n",
    "\n",
    "#### Url from attribute\n",
    "Url pointing to the document relative to the\n",
    "[docs.ton.org](https://docs.ton.org/) root path\n",
    "where applicable.\n",
    "\n",
    "#### Child nodes attribute\n",
    "Documents that are below in the document\n",
    "hierarchy than the current document.\n",
    "If document with lover heading level is encountered,\n",
    "it is being added to the parent document child nodes.\n",
    "\n",
    "#### References attribute\n",
    "Hyperlinks used in this document chunk converted to the document ids(if possible.\n",
    "If the document is part of docs-data tree, it's id will be added,\n",
    "otherwise raw url is preserved.\n",
    "\n",
    "#### Snippets attribute\n",
    "List of snippet ids, that are referenced in this document chunk.\n",
    "\n",
    "#### Document id\n",
    "Id is generated from the content and path hash:\n",
    "```python\n",
    "doc_id = hashlib.sha256(\"||\".join([content, doc_path]).encode('utf8')).hexdigest()\n",
    "```\n",
    "\n",
    "### Snippet structure\n",
    "Snippet object has the following structure:\n",
    "``` python\n",
    "codeDoc = Document(page_content=code, id=doc_id, metadata={\n",
    "        \"lang\": lang.lower(),\n",
    "        \"desc\": desc,\n",
    "        \"word_count\": len(code.split()),\n",
    "        \"token_count\": (len(code) + 3) // 4, # Rough anthropic estimate\n",
    "        \"concept\": concept,\n",
    "    })\n",
    "```\n",
    "\n",
    "#### Lang attribute\n",
    "Lowercase language name if present\n",
    "\n",
    "#### Desc attribute\n",
    "Paragraph of text that preceeds the code snippet in the document.  \n",
    "It is not guaranteed to be accurate description, but most of the time\n",
    "it works.  \n",
    "Added opportunistically.\n",
    "\n",
    "### Snippet word and token count\n",
    "Same as in document\n",
    "\n",
    "### Snippet concept attribute\n",
    "Header of the document in which the snippet is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050638b8-5c0a-4954-93a0-2a9741b0f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm beautifulsoup4 markdown2  sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219285a-988a-4cc3-8091-e43be4fc1609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, glob, tqdm\n",
    "import markdown2 as markdown\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "import unicodedata\n",
    "import hashlib\n",
    "import sys\n",
    "import re\n",
    "from langchain_core.documents import Document\n",
    "from typing import Dict, List, Optional\n",
    "from pydantic import BaseModel\n",
    "    \n",
    "docs: list[Document] = []\n",
    "snippets: Dict[str, Document] = {}\n",
    "location_map: Dict[str,str] = {}\n",
    "root_path = pathlib.Path.cwd().parent.resolve()\n",
    "sys.path.insert(0, str(root_path))\n",
    "jsx_regex = re.compile(\n",
    "    r\"\"\"^import\\s+                     # the word “import” + whitespace\n",
    "        \\{([^}]+)\\}\\s+                # everything inside the braces → group 1\n",
    "        from\\s+                       # the word “from” + whitespace\n",
    "        (['\"])(/?([^'\"]+))\\.jsx\\2     # quote (group 2), optional leading /,\n",
    "                                      # the path without extension → group 4,\n",
    "                                      # then “.jsx” and the same closing quote\n",
    "        ;?\\s+?                        # optional trailing semicolon\n",
    "    \"\"\",\n",
    "    re.VERBOSE | re.MULTILINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5945ee6-8f28-40b8-a6b0-11590ae3c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_code(code: str, lang: str, concept: str, desc: str):\n",
    "    doc_id = hashlib.sha256(\"||\".join([code, concept]).encode('utf8')).hexdigest()\n",
    "    codeDoc = Document(page_content=code, id=doc_id, metadata={\n",
    "        \"lang\": lang.lower(),\n",
    "        \"desc\": desc,\n",
    "        \"word_count\": len(code.split()),\n",
    "        \"token_count\": (len(code) + 3) // 4, # Rough anthropic estimate\n",
    "        \"concept\": concept,\n",
    "    })\n",
    "    snippets[codeDoc.id] = codeDoc\n",
    "    return codeDoc\n",
    "'''\n",
    "def create_document(content: str, doc_path: str, metadata: dict):\n",
    "    doc_id = hashlib.sha256(\"||\".join([content, doc_path]).encode('utf8')).hexdigest()\n",
    "    return Document(page_content=content, id=doc_id, metadata=metadata)\n",
    "'''\n",
    "\n",
    "def filter_jsx(content: str):\n",
    "    # Let's start with something super dumb here.\n",
    "    # Import is by far the most anoying token, rest could be ignored for now.\n",
    "    return jsx_regex.sub(\"\", content)\n",
    "def create_document(title: str, doc_chunks: List[str], path: str, references: List[str], ref_snippets: List[Dict], crumbs:List[str], children_nodes: Optional[List[str]] = None):\n",
    "    crumb_str = \">>\".join(crumbs)\n",
    "    # Put the crumbs string at the top instead of title, so the\n",
    "    # whole hierarchy participates in scoring.\n",
    "    doc_content = filter_jsx(f\"{crumb_str}\\n\\n\" + \" \".join(doc_chunks))\n",
    "    doc_id = hashlib.sha256(\"||\".join([doc_content, path]).encode('utf8')).hexdigest()\n",
    "    file_url = pathlib.Path(*(pathlib.Path(path).parts[1:]))\n",
    "    url_from = '/' + str(file_url.parent / file_url.stem)\n",
    "    \n",
    "    doc_meta = {\n",
    "                    \"concept\": title,\n",
    "                    \"word_count\": len(doc_content.split()),\n",
    "                    \"token_count\": (len(doc_content) + 3) // 4, # Rough anthropic estimate\n",
    "                    \"from\": path,\n",
    "                    \"url_from\": url_from,\n",
    "                    \"child_nodes\": children_nodes if children_nodes is not None else [],\n",
    "                    \"references\": references,\n",
    "                    \"snippets\": ref_snippets,\n",
    "                    \"crumbs\": crumb_str\n",
    "                }\n",
    "    new_doc = Document(id = doc_id, page_content=doc_content, metadata=doc_meta)\n",
    "    \n",
    "    if  url_from in location_map:\n",
    "        location_map[url_from].append(doc_id)\n",
    "    else:\n",
    "        location_map[url_from] = [doc_id]\n",
    "    for snip_ref in ref_snippets:\n",
    "        snip = snippets[snip_ref[\"id\"]]\n",
    "        snip.metadata[\"parent_doc\"] = new_doc.id\n",
    "        new_doc.metadata[\"token_count\"] += (len(snip.page_content) + 3) // 4\n",
    "        new_doc.metadata[\"word_count\"] += len(snip.page_content.split())\n",
    "        \n",
    "    return new_doc\n",
    "\n",
    "def add_to_hierarchy(docId: str, hierarchy: [str]):\n",
    "        for doc in hierarchy:\n",
    "            doc.metadata['child_nodes'].append(docId)\n",
    "\n",
    "def process_md(path: str, md_text: str, custom_title=\"\", custom_crumbs=None, skip_top = False):\n",
    "    raw_md = unicodedata.normalize(\"NFKC\", md_text)\n",
    "    #raw_md  = md_path.read_text(encoding=\"utf8\")\n",
    "    md = markdown.Markdown(extras=[\"metadata\", \"fenced-code-blocks\", \"highlightjs-lang\"])\n",
    "    text = md.convert(raw_md)\n",
    "    markup = BeautifulSoup(text, \"html.parser\")\n",
    "    title = custom_title if custom_title else md.metadata[\"title\"].strip('\"') if \"title\" in md.metadata else \"\"\n",
    "    crumbs = [title] if title else []\n",
    "    initial_crumbs = custom_crumbs if custom_crumbs else []\n",
    "    heading_level = len(crumbs)\n",
    "    chapter_length = 0\n",
    "    references = []\n",
    "    ref_snippets = []\n",
    "    chapter_chunks = []\n",
    "    last_added_text = \"\"\n",
    "    doc_hierarchy = []\n",
    "    new_docs = []\n",
    "    new_snippets = []\n",
    "    skip_set = set([\"code\", \"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "    for node in markup.descendants:\n",
    "        # skip text that is already extracted\n",
    "        if isinstance(node, NavigableString):\n",
    "            if node.parent and node.parent.name in skip_set:\n",
    "                #print(f\"Skipping {node.parent.name}\")\n",
    "                continue\n",
    "            text = str(node).strip()\n",
    "            if text:\n",
    "                chapter_length += len(text) + int(chapter_length > 0)\n",
    "                chapter_chunks.append(text)\n",
    "            continue\n",
    "        if isinstance(node, Tag) and (any(isinstance(c, str) and c.strip() for c in node.contents)):\n",
    "            #print(node.text)\n",
    "            # Potentially one of the heading tags h1 - h6\n",
    "            if len(node.name) == 2 and node.name[0] == 'h':\n",
    "                new_lvl = ord(node.name[1]) - 49 + 1 # Char codes 1 - 6 index 0 - 5\n",
    "                # Should never happen\n",
    "                if new_lvl < 0 or new_lvl > 6:\n",
    "                    continue\n",
    "                '''\n",
    "                doc_content = f\"{title}\\n\\n\" + \" \".join(chapter_chunks)\n",
    "                #print(f\"Covering concept {title}\")\n",
    "                file_url = pathlib.Path(*(pathlib.Path(path).parts[1:]))\n",
    "                url_from = '/' + str(file_url.parent / file_url.stem)\n",
    "                \n",
    "                new_doc = create_document(doc_content, path, {\n",
    "                    \"concept\": title,\n",
    "                    \"word_count\": len(doc_content.split()),\n",
    "                    \"token_count\": (len(doc_content) + 3) // 4, # Rough anthropic estimate\n",
    "                    \"from\": path,\n",
    "                    \"url_from\": url_from,\n",
    "                    \"child_nodes\": [],\n",
    "                    \"references\": references.copy(),\n",
    "                    \"snippets\": ref_snippets.copy(),\n",
    "                    \"crumbs\": \">>\".join(crumbs)\n",
    "                })\n",
    "\n",
    "                # Backward snippets mapping\n",
    "                for snip_ref in ref_snippets:\n",
    "                    snip = snippets[snip_ref.id]\n",
    "                    snip.metadata[\"parent_doc\"] = new_doc.id\n",
    "                # Add to a doc->[snippet obj] zipable list\n",
    "                \n",
    "                if  url_from in location_map:\n",
    "                    location_map[url_from].append(new_doc.id)\n",
    "                else:\n",
    "                    location_map[url_from] = [new_doc.id]\n",
    "                '''\n",
    "\n",
    "                #print(new_doc)\n",
    "                new_doc = create_document(\n",
    "                    title,\n",
    "                    doc_chunks=chapter_chunks,\n",
    "                    path=path,\n",
    "                    references=references,\n",
    "                    ref_snippets=ref_snippets,\n",
    "                    crumbs=initial_crumbs + crumbs\n",
    "                )\n",
    "                new_docs.append(new_doc)\n",
    "                #docs.append(new_doc)\n",
    "                \n",
    "                title = node.get_text(strip=True, separator=\" \").replace('\"','').strip() # Strip quote from titles for escaping simplicity\n",
    "                #print(title)\n",
    "                #print(node)\n",
    "                last_added_text = \"\"\n",
    "                chapter_text = \"\"\n",
    "                references = []\n",
    "                ref_snippets = []\n",
    "                chapter_chunks = []\n",
    "                chapter_length = 0\n",
    "                next_lvl = new_lvl > heading_level\n",
    "                prev_lvl = new_lvl < heading_level\n",
    "                #print(f\"New{new_lvl}\\n{heading_level}\")\n",
    "                #print(node)\n",
    "                heading_level = new_lvl\n",
    "                if next_lvl:\n",
    "                    crumbs.append(title)\n",
    "                    add_to_hierarchy(new_doc.id, doc_hierarchy)\n",
    "                    doc_hierarchy.append(new_doc)\n",
    "                elif prev_lvl: \n",
    "                    #print(crumbs)\n",
    "                    #print(doc_hierarchy)\n",
    "                    doc_hierarchy = doc_hierarchy[0:new_lvl - 1]\n",
    "                    add_to_hierarchy(new_doc.id, doc_hierarchy)\n",
    "                    doc_hierarchy.append(new_doc)\n",
    "                    crumbs = crumbs[0:new_lvl - 1] + [title]\n",
    "                else:\n",
    "                    crumbs[-1] = title\n",
    "                    add_to_hierarchy(new_doc.id, doc_hierarchy)\n",
    "                    \n",
    "                    '''\n",
    "                    Hierarchy is for the parents.\n",
    "                    If lelvel is not changed, should not\n",
    "                    touch it.\n",
    "                    if len(doc_hierarchy) > 0:\n",
    "                        doc_hierarchy[-1] = new_doc\n",
    "                    else:\n",
    "                        doc_hierarchy = [new_doc]\n",
    "                    '''\n",
    "                    \n",
    "                #print(f\"New crumbs {'>'.join(crumbs)} level {new_lvl}\")\n",
    "            elif node.name == \"code\":\n",
    "                code_lang = node.get('class')\n",
    "                code_text = node.text\n",
    "                lang = \"\"\n",
    "                \n",
    "                if code_lang:\n",
    "                    lang = code_lang[0]\n",
    "                # Leave inlined only oneliners with no language def\n",
    "                # Theese often can be term, and useful in vector search\n",
    "                if len(code_text.split(\"\\n\")) <= 1 and not lang:\n",
    "                    last_added_text = f\"`{code_text.strip()}`\"\n",
    "                    if last_added_text:\n",
    "                        chapter_chunks.append(last_added_text)\n",
    "                        # Data chunk length + delimiter\n",
    "                        chapter_length += len(last_added_text) + int(chapter_length > 0)\n",
    "                else:\n",
    "                    code_doc = process_code(code_text, lang, title, last_added_text)\n",
    "                    ref_snippets.append({\"id\": code_doc.id, \"pos\": chapter_length})\n",
    " \n",
    "            elif node.name == \"a\":\n",
    "                ref = node.get('href')\n",
    "                if ref:\n",
    "                    references.append(ref)\n",
    "\n",
    "    if len(title) > 0 or len(chapter_chunks) > 0:\n",
    "        #print(f\"Adding last paragraph {title}\")\n",
    "        new_doc = create_document(\n",
    "            title,\n",
    "            doc_chunks=chapter_chunks,\n",
    "            path=path,\n",
    "            references=references,\n",
    "            ref_snippets=ref_snippets,\n",
    "            crumbs= initial_crumbs + crumbs\n",
    "        )\n",
    "        add_to_hierarchy(new_doc.id, doc_hierarchy)\n",
    "        new_docs.append(new_doc)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return new_docs[1:] if skip_top else new_docs\n",
    "    \n",
    "def read_md_file(md_path: pathlib.Path, crumbs: list[str] = None):\n",
    "    doc_rel_path = md_path.relative_to(root_path)\n",
    "    print(str(doc_rel_path))\n",
    "    return process_md(str(doc_rel_path), md_path.read_text(encoding=\"utf-8\"), custom_crumbs=crumbs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a32c3-f021-44e6-a2fb-146132428bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7dcde8-5107-40eb-8904-e5cd3e57e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = root_path / \"docs-data/languages/func/known-issues.mdx\"\n",
    "crumbs = list(test_path.relative_to(root_path / \"docs-data\").parts)[:-1]\n",
    "test_docs = read_md_file(test_path, crumbs=crumbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e53818-1c5a-4b18-97e8-163d208366fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010d6ee-0281-4fba-adf8-efddf63ab425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%debug\n",
    "def count_tokens(data: str):\n",
    "    return (len(data) + 3) // 4\n",
    "    \n",
    "from core.rendering import render_docs_batch, render_single_doc\n",
    "for doc in test_docs:\n",
    "    if len(doc.metadata[\"snippets\"]) > 0:\n",
    "        #print(f\"Before {doc.page_content}\\n\\n\")\n",
    "        print(f\"Raw data tokens: {doc.metadata.get(\"token_count\", 0)}\")\n",
    "        rendered = render_single_doc(doc, doc.metadata['snippets'], snippets)\n",
    "        print(f\"Rendered token count: {count_tokens(rendered)}\")\n",
    "        print(f\"{rendered}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c45a7c-b023-42c9-bfcb-c827635dea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = root_path / \"docs-data/\"\n",
    "for file in docs_path.rglob(\"*.mdx\"):\n",
    "    print(f\"Processing {file}...\")\n",
    "    crumbs = list(file.relative_to(docs_path).parts)[:-1]\n",
    "    docs.extend(read_md_file(file, crumbs=crumbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05925f-e90e-404f-a44b-c90fe405374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add standards documentation\n",
    "TEPs = root_path / \"TEPs/text/\"\n",
    "for file in TEPs.rglob(\"*.md\"):\n",
    "    print(f\"Processing {file}...\")\n",
    "    docs.extend(read_md_file(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db6713-011b-4d2b-b1bd-ba430be6b5a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "def dump_json_docs(documents: list[Document], path: str):\n",
    "    with open(path, \"w\", encoding=\"utf8\") as out_file:\n",
    "        for doc in documents:\n",
    "            out_file.write(doc.model_dump_json() + \"\\n\")\n",
    "def load_docs_json(path: str):\n",
    "    documents = []\n",
    "    with open(path, \"r\", encoding=\"utf8\") as docs_input:\n",
    "        for doc_str in docs_input:\n",
    "            doc_parsed = json.loads(doc_str)\n",
    "            documents.append(Document(id=doc_parsed['id'], page_content=doc_parsed['page_content'], metadata = doc_parsed['metadata']))\n",
    "    return documents\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed438f-b961-4c0b-8747-b31378d74c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate document references into actual path\n",
    "for doc in docs:\n",
    "    ref_ids = []\n",
    "    for ref in doc.metadata[\"references\"]:\n",
    "        if ref in location_map:\n",
    "            ref_ids.extend(location_map[ref])\n",
    "    if len(ref_ids) > 0:\n",
    "        doc.metadata[\"references\"] = ref_ids.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e46722-c1dd-4ac1-ba24-dcc2b2ea0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_examples(examples_path: str, glob: str, concept: str, lang: str, label: str = \"\", doc_text: str = \"\"):\n",
    "    code_refs = []\n",
    "    code_docs = []\n",
    "    doc_chunks = []\n",
    "    # Hack, but whathever\n",
    "    parsed_path = pathlib.Path(examples_path)\n",
    "    if not parsed_path.is_absolute():\n",
    "        parsed_path = root_path / parsed_path\n",
    "    if doc_text:\n",
    "        doc_chunks = [doc_text]\n",
    "    '''\n",
    "    top_leval_doc = create_document(f\"{concept}\\n\", str(examples_path), {\n",
    "        \"concept\": concept,\n",
    "        \"word_count\": len(concept) + 1,\n",
    "        \"token_count\": (len(concept) + 3 + 1) // 4,\n",
    "        \"from\": str(parsed_path.relative_to(root_path)),\n",
    "        \"url_from\": \"\",\n",
    "        \"child_nodes\": code_refs,\n",
    "        \"references\": [],\n",
    "        \"snippets\": [],\n",
    "        \"crumbs\": \">>\".join([concept])\n",
    "    })\n",
    "    '''\n",
    "    top_level_doc = create_document(concept,\n",
    "                                    doc_chunks,\n",
    "                                    str(parsed_path.relative_to(root_path)),\n",
    "                                    references=[],\n",
    "                                    ref_snippets=[],\n",
    "                                    crumbs=[concept],\n",
    "                                    children_nodes=code_refs\n",
    "                                   )\n",
    "    \n",
    "    for doc in parsed_path.rglob(glob):\n",
    "        with open(doc, \"r\", encoding=\"utf8\") as example_file:\n",
    "            print(f\"File: {doc}\")\n",
    "            content = example_file.read()\n",
    "            #print(f\"Content:\\n{content}\")\n",
    "            file_label = label\n",
    "            if len(label) == 0:\n",
    "                file_label = input(\"Provide description for the source:\")\n",
    "            if file_label.lower() == \"skip\":\n",
    "                print(f\"Skipping code {doc}\")\n",
    "                continue\n",
    "            source_doc = process_code(content, lang, concept, label)\n",
    "            mention_doc = create_document(title = concept,\n",
    "                                          doc_chunks = [],\n",
    "                                          path=str(doc.relative_to(root_path)),\n",
    "                                          references=[],\n",
    "                                          ref_snippets=[{\"id\": source_doc.id, \"pos\": 0}],\n",
    "                                          crumbs=[concept, file_label]\n",
    "                                         )\n",
    "            '''\n",
    "            mention_doc = create_document(f\"{label}\\n\", str(doc), {\n",
    "                    \"concept\": file_label,\n",
    "                    \"word_count\": len(file_label) + 1,\n",
    "                    \"token_count\": (len(file_label) + 3 + 1) // 4, # Rough anthropic estimate\n",
    "                    \"from\": str(doc.relative_to(root_path)),\n",
    "                    \"url_from\": \"\",\n",
    "                    \"child_nodes\": [],\n",
    "                    \"references\": [],\n",
    "                    \"snippets\": [{\"id\": source_doc.id, \"pos\": 0}],\n",
    "                    \"crumbs\": \">>\".join([concept,file_label])\n",
    "            })\n",
    "            '''\n",
    "            code_docs.append(mention_doc)\n",
    "            code_refs.append(mention_doc.id)\n",
    "    return [top_level_doc, *code_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51625150-cff1-4e1a-8ebe-19e25dbd8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%debug\n",
    "jetton_examples = add_examples(\"tolk-contracts/contracts_Tolk/03_notcoin/\", \"*.tolk\", \"TOLK jetton contract example\", \"tolk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bff9bb-038b-4021-ab9d-45f54b00e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jetton_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed60b7c-9ee3-4550-85ee-1a083b18c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jetton_func = add_examples(\"tolk-contracts/contracts_FunC/03_notcoin/\", \"*.fc\", \"FunC jetton contract example\", \"FunC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1cd7a-9dd3-4018-83d9-5f0e25043786",
   "metadata": {},
   "outputs": [],
   "source": [
    "jetton_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66df3a-5d60-41ff-9238-bb844521b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jetton_examples.extend(jetton_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2683a21-5910-41fe-8cca-29fbe140c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nft_examples = add_examples(\"tolk-contracts/contracts_Tolk/02_nft/\", \"*.tolk\", \"TOLK NFT contract example\", \"tolk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb077a6-9428-4131-92db-9a6a7bf2b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nft_examples.extend(add_examples(\"tolk-contracts/contracts_FunC/02_nft/\",\"*.fc\", \"FunC NFT contract example\", \"FunC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf499c-ae7b-4de9-a6d3-03ea1f7cbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vesting_examples = add_examples(\"tolk-contracts/contracts_Tolk/06_vesting/\", \"*.tolk\", \"TOLK Vesting implementation\", \"tolk\")\n",
    "vesting_examples.extend(add_examples(\"tolk-contracts/contracts_FunC/06_vesting/\", \"*.fc\", \"FunC Vesting implementation\", \"FunC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108e987-a33c-47fb-903e-ce536c3743bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemint_readme=\"\"\"\n",
    "# Telemint\n",
    "This is the smart contract that Telegram intends to use in order to put some of its best usernames up for auction. The blockchain network for this smart contract is The Open Network (https://ton.org).\n",
    "\n",
    "Anyone who finds serious security vulnerabilities in this smart contract prior to the auction launch will be rewarded.\n",
    "\n",
    "## Description\n",
    "There are two smart contracts in the repository: NftCollection and NftItem.\n",
    "\n",
    "NftCollection source files: [nft-collection.fc](func/nft-collection.fc), [common.fc](func/common.fc) [stdlib.fc](func/stdlib.fc).\n",
    "\n",
    "NftItem source files: [nft-item.fc](func/nft-item.fc), [common.fc](func/common.fc) [stdlib.fc](func/stdlib.fc).\n",
    "\n",
    "One may also look at the [tlb decription](telemint.tlb) of internal messages and smart contract data.\n",
    "\n",
    "There are also two additional smart contracts in the repository: NftCollectionNoDns and NftItemNoDns. They do not support DNS and allow to set additional restrictions on first bid.\n",
    "\n",
    "NftCollectionNoDns source files: [nft-collection-no-dns.fc](func/nft-collection-no-dns.fc), [common.fc](func/common.fc) [stdlib.fc](func/stdlib.fc).\n",
    "\n",
    "NftItemNoDns source files: [nft-item-no-dns.fc](func/nft-item-no-dns.fc), [common.fc](func/common.fc) [stdlib.fc](func/stdlib.fc).\n",
    "\n",
    "### NftCollection\n",
    "\n",
    "#### Internal messages\n",
    "The first bidder receives a signed query from the server and sends it to NftCollection with the first bid attached.\n",
    "```\n",
    "// Create an NftItem and start an auction. Signed by auction's private key. Acts as a first bid in the auction.\n",
    "telemint_unsigned_deploy$_ subwallet_id:uint32 valid_since:uint32 valid_till:uint32 token_name:TelemintText\n",
    "  content:^Cell auction_config:^TeleitemAuctionConfig royalty_params:(Maybe ^NftRoyaltyParams) = TelemintUnsignedDeploy;\n",
    "telemint_msg_deploy#4637289a  sig:bits512 msg:TelemintUnsignedDeploy = TelemintMsg;\n",
    "```\n",
    "\n",
    "The NftCollection interface is also supported.\n",
    "\n",
    "#### External messages\n",
    "The smart contract will accept the first external message to simplify the initialization of the smart contract.\n",
    "\n",
    "### NftItem\n",
    "\n",
    "#### Internal messages\n",
    "The first bid is made through NftCollection, which will generate the following message.\n",
    "```\n",
    "// Create NftItem and start an auction. Accepted only from NftCollection.\n",
    "teleitem_msg_deploy#299a3e15 sender_address:MsgAddressInt bid:Grams token_info:^TelemintTokenInfo nft_content:^Cell\n",
    "  auction_config:^TeleitemAuctionConfig royalty_params:^NftRoyaltyParams = TeleitemMsg;\n",
    "```\n",
    "\n",
    "All following bids are simple transfers.\n",
    "\n",
    "The owner of an NftItem may start a new auction.\n",
    "\n",
    "```\n",
    "// Start new auction. Accepted only from the owner.\n",
    "teleitem_msg_start_auction#487a8e81 query_id:int64 auction_config:^TeleitemAuctionConfig = TeleitemMsg;\n",
    "\n",
    "// Cancel auction auction. Accepted only from the owner. Forbidden if there are some active bids\n",
    "teleitem_msg_cancel_auction#371638ae query_id:int64 = TeleitemMsg;\n",
    "```\n",
    "\n",
    "The NftItem interface is also supported, including transfer messages.\n",
    "\n",
    "#### External messages\n",
    "To finish a completed auction, one may send an empty message.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69a59f-793a-4343-8827-008b7fc387c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemint_examples = add_examples(\"tolk-contracts/contracts_FunC/07_telemint\", \"*.fc\", \"Telegram gift (telemint) contract\", \"FunC\", doc_text = telemint_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed83745-a63f-4e92-aa4c-87c8eb9557df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_examples = add_examples(\"tolk-contracts/tests\", \"*.ts\", \"Unit tests examples\", \"typescript\", doc_text=\"Collection of unit tests examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1668a76-e19e-4567-b146-28ea10cd0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_examples = add_examples(\"tolk-contracts/wrappers\", \"*.ts\", \"Contract wrappers examples\", \"typescript\", doc_text=\"Examples of a various contracts typerscript wrapper functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba658c-f91d-416a-9ab8-475b79828e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrappers_examples = add_examples(\"tolk-contracts/wrappers\", \"*.ts\", \"Contract wrappers examples\", \"typescript\", doc_text=\"Examples of a various contracts typerscript wrapper functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852ac8e-b0b1-465e-9213-1a5f00ea77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it make sense to add tolk implementation here? Probably not.\n",
    "wallet_examples = add_examples(\"tolk-contracts/contracts_FunC/05_wallet-v5/\", \"*.fc\", \"FunC wallet v5 (W5) implementation\", \"FunC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffbf59-e1d5-454b-8cd2-61c93185da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_examples = jetton_examples + tests_examples + wrapper_examples + nft_examples + vesting_examples + wallet_examples + telemint_examples\n",
    "dump_json_docs(src_examples, \"../rag-data/src_examples.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0f0d9-1f3d-468e-a963-812bd358e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1881]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb50b51-a6d4-4bb3-9af2-aae737c63d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json_docs(docs, \"../rag-data/docs_dump.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde384ac-085c-442d-a216-c1a14332c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json_docs(snippets.values(), \"../rag-data/snippets_dump.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1c769-1abd-4256-8afb-eb582dd2cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%debug\n",
    "import sys\n",
    "\n",
    "from utils.json import load_json_dict\n",
    "import re\n",
    "def fix_instruct_paragraphs(paragraphs: [str], instruction: str):\n",
    "    header_re = re.compile(r\"^(#+)\")\n",
    "    new_paragraphs = []\n",
    "    hashes_added = 0\n",
    "    paragraph_count = 0\n",
    "    for p in paragraphs:\n",
    "        #print(p)\n",
    "        header_match = header_re.match(p)\n",
    "        if header_match:\n",
    "            hashes = header_match.group()\n",
    "            total_hashes = len(hashes)\n",
    "            if hashes_added == 0 and total_hashes < 3:\n",
    "                hashes_added = 3 - total_hashes\n",
    "            if paragraph_count > 0:\n",
    "                new_paragraphs.append((\"#\" * hashes_added) + p)\n",
    "            else:\n",
    "                # Replace the first paragraph with uniq and more sensible one\n",
    "                new_paragraphs.append(f\"### {instruction}\\n instruction specification\")\n",
    "                # Make all the later paragraphs child to this one\n",
    "                hashes_added = hashes_added + 1\n",
    "            paragraph_count = paragraph_count + 1\n",
    "    return new_paragraphs\n",
    "def load_instructions():    \n",
    "    instructions_desc = load_json_dict(\"../rag-data/instructions_desc.json\")\n",
    "    instructions_docs  = []\n",
    "    for instr_cat in instructions_desc:\n",
    "        for instruction in instructions_desc[instr_cat]:\n",
    "            new_paragraphs = fix_instruct_paragraphs(instructions_desc[instr_cat][instruction].split(\"\\n\\n\"), instruction)\n",
    "            instructions_desc[instr_cat][instruction] = new_paragraphs\n",
    "            cat_title = f\"{instr_cat} instructions\"\n",
    "            instructions_docs.extend(\n",
    "                process_md(\"tvm-specification.json\",\n",
    "                           \"\\n\\n\".join(new_paragraphs),\n",
    "                           custom_title=instr_cat,\n",
    "                           custom_crumbs=['TVM instrucitons', cat_title],\n",
    "                           skip_top=True # Skip top level cat since there are hundreds of instruction in a category. Never pull them all at once\n",
    "                          )\n",
    "            )\n",
    "            #raise \"Testing\"\n",
    "    \n",
    "    return instructions_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a6844-1443-4ba0-b73d-a3368e79fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_examples = load_docs_json(root_path / \"rag-data/src_examples.jsonl\")\n",
    "old_docs = load_docs_json(root_path / \"rag-data/docs_dump.jsonl\")\n",
    "old_snippets = load_docs_json(root_path / \"rag-data/snippets_dump.jsonl\")\n",
    "docs = []\n",
    "snippets = {}\n",
    "for snip in old_snippets:\n",
    "    snippets[snip.id] = snip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9a020-50a2-496e-b6ae-9bca1d4e13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d5a0a-70ad-41fc-89e9-a7184c2411ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_docs = load_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b057f-6aec-48be-8e93-19b95de98713",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json_docs(instructions_docs, root_path / \"rag-data/instructions_documents.jsonl\")\n",
    "dump_json_docs(snippets.values(), root_path / \"instructions_snippets.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50e166-2098-4398-b3ac-a5f0559af2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_docs.extend(src_examples)\n",
    "old_docs.extend(instructions_docs)\n",
    "old_snippets.extend(snippets.values())\n",
    "dump_json_docs(path=root_path / \"rag-data/latest_docs.jsonl\", documents=old_docs)\n",
    "dump_json_docs(path=root_path / \"rag-data/latest_snippets.jsonl\", documents=old_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e4bbe-60cf-4cfb-8f9c-47f99255e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c68c3e-c1f0-4ec9-9cf6-9d4a7b1d73f5",
   "metadata": {},
   "source": [
    "# Simple documents update pipeline\n",
    "\n",
    "Documents are stored as newline separated json objects (*jsonl*).\n",
    "This means that is it editable on the spot using text editor and rebuild the index after.\n",
    "\n",
    "However, this approach won't update document hashes (id) and other attributes, unless you do it manually.\n",
    "\n",
    "Couple of snippets below allow to process documents in a more consistent fashion.\n",
    "Someday there will be an admin panel, but this day is yet to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949a1c1-5103-475b-9bb0-665e03c0bc83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recalc_doc(doc: Document, force_meta_crumbs: bool = False):\n",
    "    orig_crumbs = doc.metadata[\"crumbs\"]\n",
    "    new_crumbs  = orig_crumbs.split(\">>\")\n",
    "    new_content = doc.page_content\n",
    "    crumb_split = doc.page_content.find(\"\\n\\n\")\n",
    "    if crumb_split >= 0:\n",
    "        crumbs_found = new_content[:crumb_split]\n",
    "        # if  force_meta_crumbs is set, the crumbs from text will be overwritted by those form metadata.\n",
    "        # Otherwise crumbs found in actual document text will update the metadata.\n",
    "        if crumbs_found != orig_crumbs and (not force_meta_crumbs):\n",
    "            #print(f\"Crumbs changed {orig_crumbs} \\n New: {crumbs_found}\")\n",
    "            new_crumbs = crumbs_found.split(\">>\")\n",
    "        new_content = new_content[crumb_split + 2:]\n",
    "        \n",
    "    new_doc = create_document(\n",
    "                    doc.metadata[\"concept\"],\n",
    "                    doc_chunks=[new_content],\n",
    "                    path=doc.metadata[\"from\"],\n",
    "                    references=doc.metadata[\"references\"],\n",
    "                    ref_snippets=doc.metadata[\"snippets\"],\n",
    "                    crumbs=new_crumbs,\n",
    "                    children_nodes=doc.metadata[\"child_nodes\"]\n",
    "                )\n",
    "    return (new_doc, doc.id != new_doc.id)\n",
    "\n",
    "def recalc_batch(input_docs: list[Document]):\n",
    "    for idx, doc in enumerate(input_docs):\n",
    "        new_doc = recalc_doc(doc)\n",
    "        if new_doc[1]:\n",
    "            yield (idx, new_doc[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec266f-2881-4552-ab74-c56013bc9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_docs = load_docs_json(root_path / \"rag-data/latest_docs.jsonl\")\n",
    "cur_snippets = load_docs_json(root_path / \"rag-data/latest_snippets.jsonl\")\n",
    "\n",
    "snippets = {}\n",
    "\n",
    "for snip in cur_snippets:\n",
    "    snippets[snip.id] = snip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedf04f-7584-45d0-ba9c-45e33512c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do whatever you want with the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3e4c6-c9ac-4b57-8d0b-c06cfd9b8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-calculate the hashes and such\n",
    "new_docs = [recalc_doc(doc)[0] for doc in cur_docs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9c67e-d011-41bf-b76c-bafbfd77309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json_docs(path=root_path / \"rag-data/updated_docs.jsonl\", documents=new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729d6ec-aff9-4f4a-b680-6c47a0912c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
